import pickle as pkl
import argparse
import torch
from torch import optim, nn
import torch.utils.data as Data
from sklearn.metrics import accuracy_score, classification_report
import sys, time, os
import numpy as np
sys.path.append('../')
from model import hotspot_model
start = time.time()


#############################################################
########################  Parameters  #######################
#############################################################
parser = argparse.ArgumentParser(description="""HOTSPOT is a python library for plasmid host prediction.
                                 HOTSPOT is a Transformer-based model and rely on protein-based vocabulary to convert DNA sequences into sentences for prediction.""")
parser.add_argument('--model_path', help='folder to store your customized models, default: models', type=str, default='models')
parser.add_argument('--midfolder', help='folder to store the intermediate files (generated by preprocessing_train.py), default: temp', type=str, default='temp')
parser.add_argument('--device', help="device utilized for training ('gpu' or 'cpu'), default: 'gpu'", type=str, default = 'gpu')
parser.add_argument('--threads', help="number of threads utilized for training if 'cpu' is detected ('cuda' not found), default: 2", type=int, default=2)
parser.add_argument('--lr', help="learning rate for training the models, default: 0.005", type=float, default=0.005)
parser.add_argument('--batch_size', help="batch size for training the models, default: 200", type=int, default=200)
parser.add_argument('--epoch_num', help="number of epochs for training the models, default: 20", type=int, default=20)
parser.add_argument('--dropout', help="dropout rate for training the models, default: 0.5", type=float, default=0.5)
inputs = parser.parse_args()


#############################################################
########################  Help info  ########################
#############################################################
def help_info():
    print('')
    print("""Usage of train.py:
        [--model_path MODEL_PATH]   folder to store your customized models, default: models
        [--midfolder MIDFOLDER] folder to store the intermediate files (generated by preprocessing_train.py), default: temp
        [--device DEVICE]   device utilized for training ('gpu' or 'cpu'), default: 'gpu'
        [--threads THREADS] number of threads utilized for training if 'cpu' is detected ('cuda' not found), default: 2
        [--lr LR]   learning rate for training the models, default: 0.005
        [--batch_size BATCH_SIZE]   batch size for training the models, default: 200
        [--epoch_num EPOCH_NUM] number of epochs for training the models, default: 20
        [--dropout DROPOUT] dropout rate for training the models, default: 0.5 
    """)


#############################################################
######################  Check folders  ######################
#############################################################
out_fn = inputs.midfolder
if not os.path.exists(out_fn):
    print(f"Error! The intermediate folder '{out_fn}' is unavailable. Please use the option '--midfolder' to indicate the directory where the intermediate files generated by 'preprocessing_train.py' are stored.")
    help_info()
    sys.exit()

mdl_fn = inputs.model_path
if not os.path.exists(mdl_fn):
    print(f"Error! The customized model folder '{mdl_fn}' is unavailable. Please use the option '--model_path' to indicate the same folder as specified in 'preprocessing_train.py'.")
    help_info()
    sys.exit()


#############################################################
######################  Initialization  #####################
#############################################################
device_opt = inputs.device
if(device_opt=='gpu'):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if(device==torch.device("cuda")):
        print("Running train.py with GPU ...")
    else:
        print("CUDA is not available!", end = ' ')
else:
    device = torch.device('cpu')
if device == torch.device('cpu'):
    torch.set_num_threads(inputs.threads)    
    print(f"Running train.py with CPU {inputs.threads} threads ...")

lr = inputs.lr
batch_size = inputs.batch_size
epoch_num = inputs.epoch_num
dropout = inputs.dropout
softmax = nn.Softmax(dim=1)

feat_dict = pkl.load(open(f'{out_fn}/train_feat.dict', 'rb'))
lineage_dict = pkl.load(open(f'{mdl_fn}/train_lineage.dict', 'rb'))
level_plas = pkl.load(open(f'{mdl_fn}/level_plas.dict', 'rb'))
LABELS = pkl.load(open(f'{mdl_fn}/labels.dict', 'rb'))

def read_train_validation(path):
    tmp = set()
    f=open(path)
    ls=f.readlines()
    for l in ls:
        tmp.add(l.rstrip())
    return tmp

TRAIN = read_train_validation(f'{mdl_fn}/train.txt')
VAL = read_train_validation(f'{mdl_fn}/validation.txt')


#############################################################
##########################  Train  ##########################
#############################################################
def reset_model(label_num):
    model = hotspot_model(
    device=device,
    dropout=dropout,
    output_num = label_num
    ).to(device)

    optimizer = optim.Adam(model.parameters(), lr=lr)
    loss_func = nn.CrossEntropyLoss()
    return model, optimizer, loss_func

def return_batch(sentences, labels, flag):
    X = torch.from_numpy(sentences).to(device)
    y = torch.from_numpy(labels).to(device)
    train_dataset = Data.TensorDataset(X, y)
    training_loader = Data.DataLoader(
        dataset=train_dataset,
        batch_size=batch_size,
        shuffle=flag,
        num_workers=0,  # data will be loaded in main process
    )
    return training_loader

level_dict = {0:'Phylum', 1:'Class', 2:'Order', 3:'Family', 4:'Genus', 5:'Species'}
for level in range(6):  # phylum, class, order, family, genus, species
    X_train, X_test = [], []    # train, validation
    y_train, y_test = [], []
    labels = LABELS[level]

    # assemble dataset
    for contig in feat_dict:
        plas = contig[:contig.rfind('+')] if '+' in contig else contig
        if(plas not in level_plas[level]):
            continue
        taxon = lineage_dict[plas][level]
        label = labels[taxon]
        if(plas in TRAIN):
            X_train.append(feat_dict[contig])
            y_train.append(label)
        elif(plas in VAL):
            X_test.append(feat_dict[contig])
            y_test.append(label)
    
    X_train, X_test = np.array(X_train), np.array(X_test)
    y_train, y_test = np.array(y_train), np.array(y_test)

    # train!
    best_acc = -1
    model, optimizer, loss_func = reset_model(len(labels))
    training_loader = return_batch(X_train, y_train, flag=True)
    test_loader = return_batch(X_test, y_test, flag=False)
   
    for epoch in range(epoch_num):
        _ = model.train()
        train_correct = 0
        train_loss = []
        for step, (batch_x, batch_y) in enumerate(training_loader):
            optimizer.zero_grad()
            batch_y = batch_y.long()
            sentence = batch_x.int()
            preds = model(sentence)
            loss = loss_func(preds, batch_y)
            train_loss.append(loss.cpu().detach().numpy())
            loss.backward()
            optimizer.step()
            pred = softmax(preds).max(1, keepdim=True)[1]
            batch_y = batch_y.t()
            batch_correct = pred.eq(batch_y.view_as(pred)).sum().item()
            train_correct+=batch_correct
        train_loss = np.mean(train_loss)
        train_acc = train_correct/len(y_train)
        print(f'Model {level_dict[level]}|epoch {epoch+1}, train_acc: {train_acc}, train_loss: {train_loss}.')
        
        # validation
        _ = model.eval()
        with torch.no_grad():
            all_pred = []
            test_label = []
            test_loss = []
            for step, (batch_x, batch_y) in enumerate(test_loader):
                sentence = batch_x.int()
                preds = model(sentence)
                loss = loss_func(preds, batch_y)
                test_loss.append(loss.cpu().detach().numpy())
                preds = softmax(preds)
                pred = [torch.argmax(item) for item in preds]
                all_pred += pred
                test_label += batch_y.cpu().tolist()
            test_loss = np.mean(test_loss)
            test_label = torch.tensor(test_label)
            all_pred = torch.tensor(all_pred)
            test_label = test_label.long()
            accuracy = accuracy_score(test_label, all_pred)
            print(f'Model {level_dict[level]}|epoch {epoch+1}, val_acc: {accuracy}, val_loss: {test_loss}')
            print(classification_report(test_label, all_pred))
            if(accuracy<=train_acc):
                if(accuracy>best_acc):
                    best_acc=accuracy
                    print(f'Model {level_dict[level]} achieves the best accuracy {best_acc} in epoch {epoch+1}!')
                    torch.save(model.state_dict(), f'{mdl_fn}/{level_dict[level]}.pth')
    print(f"The training of model {level_dict[level]} finished. The best accuracy is {best_acc}.")

end = time.time()
print(f'Total running time of model training is {end-start}s.')
